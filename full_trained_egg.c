#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>
#include <math.h>
#include <string.h>
#include <time.h>

// --- Configuration [cite: 275, 277, 288] ---
#define VOCAB_SIZE 256        // Byte-level tokenization
#define HIDDEN_DIM 256        // Model width
#define N_LAYERS 4            // Number of layers
#define SEQ_LEN 64            // Sequence length for BPTT (truncated)
#define POPULATION_SIZE 64    // Number of perturbations per step
#define BATCH_SIZE 8          // Parallel streams
#define FIXED_POINT 4         // 4 bits for fractional part
#define SIGMA_SHIFT 4         // Noise scale (bitwise shift)
#define UPDATE_THRESHOLD 100  // Votes needed to flip a weight [cite: 1023]
#define MAX_VAL 127
#define MIN_VAL -127

// --- Lookup Tables [cite: 998-1000] ---
int32_t LOG2_TABLE[1024];
int32_t EXP2_TABLE[256];

// --- Data Structure ---
typedef struct {
    uint8_t *data;
    long length;
} Dataset;

// --- Model Parameters Struct ---
// Storing all weights in a flat structure for easier iteration
typedef struct {
    int8_t embedding[VOCAB_SIZE * HIDDEN_DIM];
    // Layers: We will flatten them logic-wise into arrays for the update loop
    // 0: Wf, 1: Uf, 2: Wh, 3: Uh (GRU) | 4: W1, 5: W2 (MLP) | 6: Head
    // We simplify LN weights to be fixed or separately managed to reduce code size
    int8_t gru_weights[N_LAYERS][4][HIDDEN_DIM * HIDDEN_DIM];
    int8_t gru_biases[N_LAYERS][2][HIDDEN_DIM]; // 0: bf, 1: bh
    int8_t mlp_weights[N_LAYERS][2][HIDDEN_DIM * (HIDDEN_DIM * 4)]; // 0: Expand, 1: Project
    int8_t head[HIDDEN_DIM * VOCAB_SIZE];
    int8_t ln_weights[N_LAYERS][2][HIDDEN_DIM]; // 0: LN1, 1: LN2
    int8_t ln_out[HIDDEN_DIM];
} EggModel;

// --- Helper Functions ---

void init_tables() {
    for(int i=0; i<256; i++) 
        EXP2_TABLE[i] = (int32_t)(pow(2.0, (double)i / (1 << FIXED_POINT)) * (1 << FIXED_POINT));
    for(int i=0; i<1024; i++) {
        if(i==0) LOG2_TABLE[i] = 0;
        else LOG2_TABLE[i] = (int32_t)(log2((double)i / (1 << FIXED_POINT)) * (1 << FIXED_POINT));
    }
}

// Saturated Addition [cite: 283, 936]
int8_t clipped_add(int32_t a, int32_t b) {
    int32_t res = a + b;
    if (res > MAX_VAL) return MAX_VAL;
    if (res < MIN_VAL) return MIN_VAL;
    return (int8_t)res;
}

// RNG helpers
uint32_t xorshift32(uint32_t *state) {
    uint32_t x = *state;
    x ^= x << 13;
    x ^= x >> 17;
    x ^= x << 5;
    *state = x;
    return x;
}

int8_t gen_noise_val(uint32_t *rng) {
    return (int8_t)((xorshift32(rng) & 31) - 16);
}

// --- The Core "Rank-1 Perturbed Matrix Mul" [cite: 68, 1009] ---
// Computes x(W + s*AB^T)^T = xW^T + s(xB)A^T
// If noise_sign is 0, it performs standard inference.
void matmul_perturbed(
    const int8_t *in, const int8_t *w, int8_t *out, 
    int rows, int cols, 
    uint32_t layer_seed, int noise_sign
) {
    // 1. Generate Noise Vectors A and B on the fly
    // To match JAX, we would need the exact same RNG, here we just ensure consistency
    int8_t *A = (int8_t*)malloc(rows);
    int8_t *B = (int8_t*)malloc(cols);
    uint32_t rng = layer_seed;
    for(int i=0; i<rows; i++) A[i] = gen_noise_val(&rng);
    for(int i=0; i<cols; i++) B[i] = gen_noise_val(&rng);

    // 2. Compute xB (Projection onto B)
    int32_t xB = 0;
    for(int j=0; j<cols; j++) xB += (int32_t)in[j] * (int32_t)B[j];

    // 3. Compute Result
    int shift = 8; // Division by 16*sqrt(n) approx
    for(int i=0; i<rows; i++) {
        int32_t acc = 0;
        for(int j=0; j<cols; j++) acc += (int32_t)in[j] * (int32_t)w[i * cols + j];
        
        if (noise_sign != 0) {
            int32_t noise = (xB * (int32_t)A[i]) * noise_sign;
            acc += (noise >> SIGMA_SHIFT);
        }
        
        int32_t res = acc >> shift;
        if(res > MAX_VAL) out[i] = MAX_VAL;
        else if(res < MIN_VAL) out[i] = MIN_VAL;
        else out[i] = (int8_t)res;
    }
    free(A); free(B);
}

// --- Layer Norm [cite: 965] ---
void egg_ln(const int8_t *x, const int8_t *w, int8_t *out) {
    int32_t sum = 0;
    for(int i=0; i<HIDDEN_DIM; i++) sum += abs(x[i]);
    if(sum == 0) sum = 1;
    int32_t mean = sum / HIDDEN_DIM;
    if(mean == 0) mean = 1;
    
    for(int i=0; i<HIDDEN_DIM; i++) {
        int32_t val = ((int32_t)x[i] * w[i]) / mean;
        if(val > MAX_VAL) out[i] = MAX_VAL;
        else if(val < MIN_VAL) out[i] = MIN_VAL;
        else out[i] = (int8_t)val;
    }
}

// --- Forward Pass (With Noise Injection) [cite: 915] ---
void forward_pass(
    EggModel *model, 
    const uint8_t *inputs, 
    int seq_len, 
    int8_t *logits_out, 
    uint32_t step_seed, 
    int noise_sign
) {
    int8_t x[HIDDEN_DIM], residual[HIDDEN_DIM], state[N_LAYERS][HIDDEN_DIM];
    int8_t buf1[HIDDEN_DIM], buf2[HIDDEN_DIM], buf3[HIDDEN_DIM]; // scratch pads
    memset(state, 0, sizeof(state));

    for(int t=0; t<seq_len; t++) {
        // Embedding (Perturbed)
        // Simplification: We only perturb embedding via the linear projection logic if implemented as matmul
        // Here simple lookup:
        memcpy(x, &model->embedding[inputs[t] * HIDDEN_DIM], HIDDEN_DIM);

        // Layers
        for(int l=0; l<N_LAYERS; l++) {
            uint32_t l_seed = step_seed + (l * 100); // Unique seed per layer

            // -- GRU --
            memcpy(residual, x, HIDDEN_DIM);
            egg_ln(x, model->ln_weights[l][0], x);

            // Wf, Uf -> f_gate
            matmul_perturbed(x, model->gru_weights[l][0], buf1, HIDDEN_DIM, HIDDEN_DIM, l_seed+1, noise_sign);
            matmul_perturbed(state[l], model->gru_weights[l][1], buf2, HIDDEN_DIM, HIDDEN_DIM, l_seed+2, noise_sign);
            // ft = sigmoid(Wf + Uf + bf) -> Identity
            int8_t ft[HIDDEN_DIM];
            for(int i=0; i<HIDDEN_DIM; i++) ft[i] = clipped_add(clipped_add(buf1[i], buf2[i]), model->gru_biases[l][0][i]);

            // Gated Past
            int8_t gated_past[HIDDEN_DIM];
            for(int i=0; i<HIDDEN_DIM; i++) gated_past[i] = (int8_t)(((int32_t)(ft[i] + 127) * state[l][i]) >> 8);

            // Wh, Uh -> h_cand
            matmul_perturbed(x, model->gru_weights[l][2], buf1, HIDDEN_DIM, HIDDEN_DIM, l_seed+3, noise_sign);
            matmul_perturbed(gated_past, model->gru_weights[l][3], buf2, HIDDEN_DIM, HIDDEN_DIM, l_seed+4, noise_sign);
            int8_t ht[HIDDEN_DIM];
            for(int i=0; i<HIDDEN_DIM; i++) ht[i] = clipped_add(clipped_add(buf1[i], buf2[i]), model->gru_biases[l][1][i]);

            // State Update
            for(int i=0; i<HIDDEN_DIM; i++) {
                int32_t update = ((int32_t)(ft[i] + 127) * (ht[i] - state[l][i])) >> 8;
                state[l][i] = clipped_add(state[l][i], update);
                x[i] = state[l][i]; // Output is new state
            }
            
            // Residual Add
            for(int i=0; i<HIDDEN_DIM; i++) x[i] = clipped_add(x[i], residual[i]);

            // -- MLP --
            memcpy(residual, x, HIDDEN_DIM);
            egg_ln(x, model->ln_weights[l][1], x);
            
            // Expansion (In: HIDDEN, Out: 4*HIDDEN)
            // We use a simplified MLP (Hidden -> Hidden) to save C-code complexity/memory here
            // or split the large matrix into chunks. 
            // For brevity: using Hidden->Hidden MLP.
            matmul_perturbed(x, model->mlp_weights[l][0], buf1, HIDDEN_DIM, HIDDEN_DIM, l_seed+5, noise_sign);
            matmul_perturbed(buf1, model->mlp_weights[l][1], x, HIDDEN_DIM, HIDDEN_DIM, l_seed+6, noise_sign);

            for(int i=0; i<HIDDEN_DIM; i++) x[i] = clipped_add(x[i], residual[i]);
        }

        // Final Head
        egg_ln(x, model->ln_out, x);
        // Compute logits only for the last token if seq_len > 1 (simplification for loss calc)
        // or for every token if doing full sequence training.
        // We output the last one for batch loss calculation.
        matmul_perturbed(x, model->head, logits_out, VOCAB_SIZE, HIDDEN_DIM, step_seed+999, noise_sign);
    }
}

// --- Loss Calculation [cite: 994-997] ---
int32_t compute_loss(int8_t *logits, uint8_t target) {
    int32_t sum_exp = 0;
    for(int i=0; i<VOCAB_SIZE; i++) {
        int idx = (int32_t)logits[i] + 128;
        sum_exp += EXP2_TABLE[idx < 0 ? 0 : (idx > 255 ? 255 : idx)];
    }
    int32_t log_sum = (sum_exp < 1024) ? LOG2_TABLE[sum_exp] : LOG2_TABLE[1023];
    int32_t target_logit = (int32_t)logits[target] + 128;
    return log_sum - target_logit;
}

// --- Parameter Update (The "Vote") [cite: 1019-1023] ---
void update_matrix(
    int8_t *W, int rows, int cols, 
    uint32_t seed, 
    const int *fitnesses, // Array of fitness values per population member
    int pop_size
) {
    int32_t *votes = (int32_t*)calloc(rows * cols, sizeof(int32_t));
    int8_t *A = (int8_t*)malloc(rows);
    int8_t *B = (int8_t*)malloc(cols);

    // Reconstruct Noise and Accumulate Votes
    for(int p=0; p<pop_size; p+=2) {
        uint32_t p_seed = seed + (p/2); // Same seed logic as training loop
        uint32_t rng = p_seed;
        for(int i=0; i<rows; i++) A[i] = gen_noise_val(&rng);
        for(int i=0; i<cols; i++) B[i] = gen_noise_val(&rng);

        int f = fitnesses[p/2]; // Fitness for this antithetical pair
        
        // Vote = f * (AB^T)
        for(int i=0; i<rows; i++) {
            for(int j=0; j<cols; j++) {
                votes[i*cols + j] += f * ((int32_t)A[i] * B[j]);
            }
        }
    }

    // Apply Update
    for(int i=0; i<rows*cols; i++) {
        if(votes[i] > UPDATE_THRESHOLD && W[i] < MAX_VAL) W[i]++;
        else if(votes[i] < -UPDATE_THRESHOLD && W[i] > MIN_VAL) W[i]--;
    }
    free(votes); free(A); free(B);
}

// --- Main Training Loop ---

Dataset load_data(const char *filename) {
    FILE *f = fopen(filename, "rb");
    if(!f) { printf("Error: Create 'input.txt' first.\n"); exit(1); }
    fseek(f, 0, SEEK_END);
    long len = ftell(f);
    fseek(f, 0, SEEK_SET);
    uint8_t *data = (uint8_t*)malloc(len);
    fread(data, 1, len, f);
    fclose(f);
    return (Dataset){data, len};
}

int main() {
    init_tables();
    Dataset ds = load_data("input.txt");
    printf("Loaded dataset: %ld bytes\n", ds.length);

    // Init Model (Random)
    EggModel *model = (EggModel*)calloc(1, sizeof(EggModel));
    uint32_t init_rng = 42;
    for(int i=0; i<VOCAB_SIZE*HIDDEN_DIM; i++) model->embedding[i] = gen_noise_val(&init_rng);
    for(int i=0; i<HIDDEN_DIM*VOCAB_SIZE; i++) model->head[i] = gen_noise_val(&init_rng);
    // (Initialize other weights similarly... omitted for brevity)
    for(int l=0; l<N_LAYERS; l++) {
        for(int i=0; i<HIDDEN_DIM; i++) model->ln_weights[l][0][i] = 16; // Init LN scale
        for(int i=0; i<HIDDEN_DIM; i++) model->ln_weights[l][1][i] = 16;
        for(int i=0; i<HIDDEN_DIM; i++) model->ln_out[i] = 16;
    }

    int *pair_fitnesses = (int*)malloc((POPULATION_SIZE/2) * sizeof(int));
    int8_t *logits = (int8_t*)malloc(VOCAB_SIZE);

    printf("Starting EGGROLL Training...\n");

    for(int step=0; step<1000; step++) {
        uint32_t step_seed = (uint32_t)time(NULL) + step;
        
        // 1. Evaluate Population
        // We use antithetical pairs: (seed, +1) and (seed, -1)
        for(int p=0; p < POPULATION_SIZE; p+=2) {
            uint32_t p_seed = step_seed + (p/2); 
            
            // Pick a random batch location
            int start_idx = rand() % (ds.length - SEQ_LEN - 1);
            
            // Pos Run
            forward_pass(model, &ds.data[start_idx], SEQ_LEN, logits, p_seed, 1);
            int32_t loss_pos = compute_loss(logits, ds.data[start_idx + SEQ_LEN]);
            
            // Neg Run
            forward_pass(model, &ds.data[start_idx], SEQ_LEN, logits, p_seed, -1);
            int32_t loss_neg = compute_loss(logits, ds.data[start_idx + SEQ_LEN]);

            // Fitness Shaping [cite: 1014-1017]
            if (loss_pos < loss_neg) pair_fitnesses[p/2] = 1;
            else if (loss_neg < loss_pos) pair_fitnesses[p/2] = -1;
            else pair_fitnesses[p/2] = 0;
        }

        // 2. Update Weights
        // Iterate over all layers and update
        for(int l=0; l<N_LAYERS; l++) {
            uint32_t l_seed = step_seed + (l * 100);
            update_matrix(model->gru_weights[l][0], HIDDEN_DIM, HIDDEN_DIM, l_seed+1, pair_fitnesses, POPULATION_SIZE); // Wf
            update_matrix(model->gru_weights[l][1], HIDDEN_DIM, HIDDEN_DIM, l_seed+2, pair_fitnesses, POPULATION_SIZE); // Uf
            update_matrix(model->gru_weights[l][2], HIDDEN_DIM, HIDDEN_DIM, l_seed+3, pair_fitnesses, POPULATION_SIZE); // Wh
            update_matrix(model->gru_weights[l][3], HIDDEN_DIM, HIDDEN_DIM, l_seed+4, pair_fitnesses, POPULATION_SIZE); // Uh
            
            update_matrix(model->mlp_weights[l][0], HIDDEN_DIM, HIDDEN_DIM, l_seed+5, pair_fitnesses, POPULATION_SIZE); // MLP1
            update_matrix(model->mlp_weights[l][1], HIDDEN_DIM, HIDDEN_DIM, l_seed+6, pair_fitnesses, POPULATION_SIZE); // MLP2
        }
        update_matrix(model->head, VOCAB_SIZE, HIDDEN_DIM, step_seed+999, pair_fitnesses, POPULATION_SIZE);

        if(step % 10 == 0) printf("Step %d complete.\n", step);
    }

    printf("Training Done.\n");
    free(ds.data); free(model); free(logits); free(pair_fitnesses);
    return 0;
}
